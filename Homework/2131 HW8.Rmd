---
title: ""
author: ""
date: ""
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
setwd("C:/Users/orlyo/OneDrive/Desktop/Grad School/Fall 2020/STAT 2131 - Applied Statistical Methods 1/Homeworks/HW8")
gamble = read.csv("Gambling.csv")
```

# Question 2

*(a) Regress gamble onto the other four predictors. Do you see any evidence that the mean model or constant variance assumption is violated? Are the errors normally distributed?*

```{r, fig.width = 5, fig.height = 3}
model = lm(gamble ~ sex + status + income + verbal, data = gamble)
summary(model)
par(mfrow = c(1, 2))
plot(gamble$gamble, model$residuals, main = "model residuals", xlab = "gamble", 
     ylab = "residuals")
abline(h = 0,col = "red")
plot(gamble$gamble, abs(model$residuals), main = "variance investigation", xlab = "gamble", 
     ylab = "|residuals|")
abline(h = 0, col = "red")
```

From the residual plot we can see a potential outlier and a potential for the data not to fit a linear model very well. A plot of the absolute value of the residuals shows non-constant variance as well, and we will have to do something about that. The summary tells us similar things - the R^2 is not ideal, and only 2 of the predictors are significant at an alpha of 0.05.

*(b) Use Box Cox with gamma > 0 to suggest a transformation of the response, ˜Y , so that ˜Y satisfies the usual mean and variance assumptions. Plot ˆ˜Y vs. the estimated residuals. Does your new model appear to satisfy the constant variance assumption? Does ˜Y appear to be normally distributed? (Hint: the Box Cox example code can be found in Transformations.Rmd on blackboard. You may need to add a small delta > 0 to gamble to get the function to work, since the function requires Y > 0. delta can be chosen to be arbitrarily small, like 10^-8).*

```{r, fig.width = 5, fig.height = 3}
gamble$gamble2 = gamble$gamble + 0.0000001 # correct Y to satisfy boxcox requirements
model2 = lm(gamble2 ~ sex + status + income + verbal, data = gamble)

par(mfrow = c(1, 1))
bc = boxcox(model2)
mtext("BoxCox", 3)
plot(fitted(model2), resid(bc), main = "Fitted model vs. Residuals")
abline(h = 0,col = "red")

bc2 = boxcox(model2, lambda = seq(.1, .3, by = .01))
lambda = .21
Y.tilde = ((gamble$gamble2^lambda) - 1)/lambda
plot(Y.tilde, resid(bc), main = "Y˜ vs. Model residuals")
abline(h = 0,col = "red")
```

The first graph is the boxcox function onto the new model that incorporates the non-zero Y values (just barely non-zero, to comply with boxcox model requirements). The second plot graphs the residuals against the fitted values, and we can see that they are slightly slanted up but much better than before. Now we can analyze the boxcox interval to determine a lambda appropriate for transforming Y. The next plot shows this interval, and lambda can be narrowed down to about .21. Y˜ is calculated using this lambda and the next plot shows the model residuals plotted against the transformed Y, Y˜, and we can see that the errors are far less correlated.

*(c) Compute the hat matrix and plot a histogram of the leverage scores.*

```{r, fig.width = 5, fig.height = 3}
model2 = lm(gamble2 ~ sex + status + income + verbal, data = gamble)
X = model.matrix(model2)
hat_matrix = X%*%(solve(t(X)%*%X)%*%t(X))
diag(hat_matrix)

hist(diag(hat_matrix))
```

*(i) Why should one be concerned if there are any abnormally large leverage scores? Do you see any evidence of large leverage points in these data?*

Abnormally large leverage scores will potentially affect the model fit in a few different ways. If it is an outlier in Y, it may be further away from the desired fit and sway the regression line in one way or another. If it is an outlier in X, it may not effect the model itself, but is still far away from most of the data. If it is an outlier in both X and Y, the regression line will cross it and it still sways the linear regression fit. We can use the leverage scores, or the diagonals of the hat matrix, to determine these outliers. The histogram above shows a potential large leverage score in the .30-.35 box, since it is further from the main group of the data. We can use the leverage score/s of this point/s to diagnose a potential outlier, and determine what if anything should be done about it.

*(ii) Re-estimate the model from part (b) after removing the points with leverage scores > 2p/n. Do the parameter estimates or standard errors change substantially?*

2p/n is equal to twice the average of the leverage scores. Here, that is:
```{r}
2*mean(diag(hat_matrix))
p = 5
n = 47
2*p/n
```

Now we can remove the points with leverage scores above this value and re-estimate the model. Observations 31, 33, 35, and 42 have leverage scores above the threshhold.

```{r, fig.width = 5, fig.height = 3}
diag(hat_matrix) > 2*p/n
which(diag(hat_matrix) > 2*p/n)
adj.data = gamble[-c(31, 33, 35, 42),]

model3 = lm(gamble ~ sex + status + income + verbal, data = adj.data)
summary(model3)
X.adj = model.matrix(model3)
hat_matrix_adj = X.adj%*%(solve(t(X.adj)%*%X.adj)%*%t(X.adj))
diag(hat_matrix_adj)
hist(diag(hat_matrix_adj))

coef(summary(model2))[, 1]; coef(summary(model3))[, 1] # coefficients
coef(summary(model2))[, 2]; coef(summary(model3))[, 2] # standard errors
```

It does not appear that the coefficients or the standard errors change too much after removing the points that have large leverage scores. The intercept changes the most (magnitude), and verbal changes slightly, but coefficients appear to stay in the same direction and generally of the same magnitude after removing the points.

*(d) Compute the Cook’s distance for each of the n points. Do any of the points appear to be influential points?*

For Cook's distance, we might use a cutoff of 4/(n-p), and determine that the points with distances over this value are potentially influential to the model fit. We can first look at a histogram of the distances (similar to the leverage scores - it's always helpful to SEE the data).

```{r, fig.width = 5, fig.height = 5}
4/(n-p)
par(mfrow = c(1, 1))

cooksd = cooks.distance(model)
plot(cooksd, pch = "*", cex = 2, main = "Influential Obs by Cooks distance")
abline(h = 4*mean(cooksd, na.rm = TRUE), col = "red")
text(x = 1:length(cooksd) + 1, y = cooksd, labels = ifelse(cooksd > 4/(n-p), names(cooksd), ""), col = "red")
```

The plot shows that one point is very far away from the rest of the data. The red line shows the cutoff of 4/(n-p), and we can see that observations 24 and 39 are potentially influential based on Cook's distance diagnostics.

\newpage

# Problem 3 Part (c)

*For this problem, choose your own word or phrase that you consider interesting, and fit a non parametric local linear regression to the word frequency over time.*

*Plot your estimated function on top of the raw data and report how you chose the bandwidth h, the effective degrees of freedom of your fitted function and if your fit was dependent on the choice of kernel K.*

``` {r, fig.width = 5, fig.height = 4}
ngram = read.csv("ngram_data.csv")

dens = density(ngram$year, kernel = "gaussian")
bw = dens$bw

plot(ngram$year, ngram$ngram, type = "l", 
     main = " 'first amendment' over time with kernel smoother ")
lines(ksmooth(ngram$year, ngram$ngram, "normal", bandwidth = bw), col = "red", lwd = 2)
```

Using the density() function, I found the gaussian kernel and subsequent bandwidth based on the year. The function ksmooth then takes in this bandwidth and graphs a fitted model on top, depending on this gaussian kernel. 

The degrees of freedom is the trace of the hat matrix, in this case 1.

``` {r}
X = ngram$year
H = X %*% solve(t(X) %*% X) %*% t(X)
sum(diag(H))
```
